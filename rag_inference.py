import os

os.environ["OPENAI_API_BASE"] = "http://192.168.2.87:8000/v1"
os.environ["OPENAI_API_KEY"] = "api-fake"

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
MODEL_ID = "Tlite"
TOKEN = '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'
DATA_PATH = 'rp/witcher3_knowledge_base.json'

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_milvus import Milvus

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ Milvus
vectorstore = Milvus(
    embedding_function=embeddings,
    collection_name="witcher_3rag",
    connection_args={"host": "localhost", "port": "19530"}
)

retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 3}
)

# LLM —á–µ—Ä–µ–∑ vLLM
llm = ChatOpenAI(
    model_name=MODEL_ID,
    openai_api_base=os.environ["OPENAI_API_BASE"],
    openai_api_key=os.environ["OPENAI_API_KEY"],
    temperature=0.6,
    #max_tokens=1024,
)


def format_docs_with_sources(docs):
    result = ""
    sources = set()
    for doc in docs:
        result += doc.page_content + "\n\n"
        sources.add(doc.metadata["url"])
    return result.strip(), list(sources)

# –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π retriever
def retrieve_with_sources(question):
    docs = retriever.invoke(question)
    context, sources = format_docs_with_sources(docs)
    
    # üîç –û–¢–õ–ê–î–ö–ê: –≤—ã–≤–æ–¥–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
    print("\n" + "="*80)
    print("üîç –ù–ê–ô–î–ï–ù–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:")
    print("="*80)
    print(context)
    print("\nüìö –ò–°–¢–û–ß–ù–ò–ö–ò:")
    for src in sources:
        print(f"  - {src}")
    print("="*80 + "\n")
    
    return {"context": context, "sources": sources, "question": question}

# –ù–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
template_with_sources = """–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏: "–Ø –Ω–µ –∑–Ω–∞—é –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö".

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""

prompt = ChatPromptTemplate.from_template(template_with_sources)

# –¶–µ–ø–æ—á–∫–∞ —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
rag_chain_with_sources = (
    retrieve_with_sources
    | prompt
    | llm
    | StrOutputParser()
)

# –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
result = rag_chain_with_sources.invoke("–ö—Ç–æ —Ç–∞–∫–æ–π –û–ª—å–≥–µ—Ä–¥?")
#print(rag_chain_with_sources)
print(result)