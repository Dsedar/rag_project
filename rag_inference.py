import os
import logging
import requests
from typing import Dict, List, Any, ClassVar
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_milvus import Milvus
from sentence_transformers import CrossEncoder
from pymilvus import Collection, connections

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
MODEL_ID = "Tlite"
TOKEN = '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'
DATA_PATH = 'rp/witcher3_knowledge_base.json'
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "witcher3_rag"

os.environ["OPENAI_API_BASE"] = "http://192.168.2.87:8000/v1"
os.environ["OPENAI_API_KEY"] = "api-fake"

class HybridMilvusRetriever(BaseRetriever):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ—Ç—Ä–∏–≤–µ—Ä: —Å–Ω–∞—á–∞–ª–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, –∑–∞—Ç–µ–º rerank –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–æ–º.
    –°–æ–≤–º–µ—Å—Ç–∏–º —Å LangChain.
    """
    vectorstore: Any
    collection: Any  # ‚Üê –î–û–ë–ê–í–õ–Ø–ï–ú! –ù–∞—Ç–∏–≤–Ω—ã–π pymilvus.Collection
    embedding_function: Any
    reranker_model_name: str = "BAAI/bge-reranker-large"
    k: int = 4
    fetch_k: int = 20
    ef: int = 40  # ‚Üê –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ expanded_search!
    all_categories: ClassVar[List[str]] = []
    _reranker: CrossEncoder = None

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._reranker = CrossEncoder(self.reranker_model_name)
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω reranker: {self.reranker_model_name}")

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        try:
            # –ü–µ—Ä–µ–¥–∞—ë–º collection, –∞ –Ω–µ vectorstore!
            top_hits = expanded_search(
                collection=self.collection,  # ‚Üê –í–ê–ñ–ù–û!
                all_categories=self.all_categories,
                original_query=query,
                model=self.embedding_function,
                reranker=self._reranker,  # ‚Üê –ü–ï–†–ï–î–ê–Å–ú, —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –∑–∞–Ω–æ–≤–æ
                k=self.fetch_k,
                ef=self.ef  # ‚Üê –ü–ï–†–ï–î–ê–Å–ú ef!
            )

            docs = [
                Document(
                    page_content=hit.entity.get("text"),
                    metadata={
                        "title": hit.entity.get("title"),
                        "url": hit.entity.get("url"),
                        "subcategory": hit.entity.get("subcategory"),
                        "distance": hit.distance
                    }
                )
                for hit in top_hits
            ]
            logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∞ —ç—Ç–∞–ø–µ dense –ø–æ–∏—Å–∫–∞.")

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π reranker –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            if self._reranker and len(docs) > self.k:
                pairs = [(query, doc.page_content) for doc in docs]
                scores = self._reranker.predict(pairs)
                scored_docs = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)
                docs = [doc for doc, score in scored_docs[:self.k]]

            return docs

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤–µ–∫—Ç–æ—Ä–Ω–æ–º –ø–æ–∏—Å–∫–µ: {e}")
            return []

def connect_to_milvus():
    try:
        vectorstore = Milvus(
            embedding_function=embeddings,
            collection_name=COLLECTION_NAME,
            connection_args={"host": MILVUS_HOST, "port": MILVUS_PORT},
            primary_field="id",
            text_field="text",
            vector_field="vector"
        )
        return vectorstore
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Milvus: {e}")
        raise

def get_unique_subcategories(collection: Collection, limit: int = None) -> list:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å—Ç–æ–ª–±—Ü–∞ `subcategory`.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –ø–æ–ª—è –Ω—É–∂–Ω–æ –∏–∑–≤–ª–µ—á—å
    output_fields = ["subcategory"]

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∑–∞–ø–∏—Å–∏ (–∏–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ)
    if limit:
        results = collection.query(
            expr="",  # –ü—É—Å—Ç–æ–π expr –æ–∑–Ω–∞—á–∞–µ—Ç "–≤—Å–µ –∑–∞–ø–∏—Å–∏"
            output_fields=output_fields,
            limit=limit
        )
    else:
        # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –º–Ω–æ–≥–æ, –ª—É—á—à–µ –∏–∑–≤–ª–µ–∫–∞—Ç—å –ø–æ—Ä—Ü–∏—è–º–∏
        offset = 0
        batch_size = 1000
        results = []
        while True:
            batch = collection.query(
                expr="",
                output_fields=output_fields,
                offset=offset,
                limit=batch_size
            )
            if not batch:
                break
            results.extend(batch)
            offset += batch_size

    # –í—ã–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    unique_subcategories = set()
    for record in results:
        subcategory = record.get("subcategory")
        if subcategory:
            unique_subcategories.add(subcategory)

    return list(unique_subcategories)

def format_docs_with_sources(docs):
    result = ""
    sources = set()
    categories = set()
    for doc in docs:
        result += f"[–ù–∞—á–∞–ª–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n{doc.page_content}\n[–ö–æ–Ω–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞]\n"
        sources.add(doc.metadata.get("url", "–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω"))
        categories.add(doc.metadata.get("subcategory", "–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ—Ç"))
    return result.strip(), list(sources), list(categories)

def retrieve_with_sources(question: str) -> Dict:
    try:
        docs = retriever.invoke(question)
        context, sources , categories = format_docs_with_sources(docs)

        logger.info(f"–ù–∞–π–¥–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞: {question}")
        logger.info(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç: {context}")  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–≤—ã—Ö 200 —Å–∏–º–≤–æ–ª–æ–≤
        logger.info(f"–ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}")
        logger.info(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {categories}")

        return {"context": context, "sources": sources, "categories": categories, "question": question}
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
        return {"context": "", "sources": [],  "categories": [], "question": question}

def generate_paraphrases(query: str, num_paraphrases: int = 3) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é LLM.
    """
    url = "http://192.168.2.87:8000/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    prompt = f"""
    –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å {num_paraphrases} —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏:
    –ó–∞–ø—Ä–æ—Å: {query}
    –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
    """
    data = {
        "model": "Tlite",
        "messages": [{"role": "user", "content": prompt}],
        "temperature": 0.7,
        "max_tokens": 200
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        paraphrases = response.json()["choices"][0]["message"]["content"].strip().split("\n")
        print(f"–ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {[p.strip() for p in paraphrases if p.strip()]}")
        return [p.strip() for p in paraphrases if p.strip()]
    else:
        return [query]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å, –µ—Å–ª–∏ —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫

def generate_related_queries(query: str, categories: List[str]) -> List[str]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞.
    """
    url = "http://192.168.2.87:8000/v1/chat/completions"
    headers = {"Content-Type": "application/json"}
    prompt = f"""
    –í—ã–±–µ—Ä–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ {categories} 3 –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥—É—Ç –≤ –ø–æ–∏—Å–∫–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç—Ç–æ–º—É –∑–∞–ø—Ä–æ—Å—É:
    –ó–∞–ø—Ä–æ—Å: {query}
    –û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º —ç—Ç–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π, —Ç–∞–∫ –∫–∞–∫ –æ–Ω–∏ –∑–∞–ø–∏—Å–∞–Ω—ã, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
    """
    data = {
        "model": "Tlite",
        "messages": [{"role": "user", "content": prompt}],
        #"temperature": 0.7,
        "max_tokens": 400
    }
    response = requests.post(url, headers=headers, json=data)
    if response.status_code == 200:
        relevant_categories = response.json()["choices"][0]["message"]["content"].strip().split("\n")
        print(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã: {[c.strip() for c in relevant_categories if c.strip()]}")
        return [c.strip() for c in relevant_categories if c.strip()]
    else:
        return [query] 

def expanded_search(
    collection,
    all_categories: List[str],
    original_query: str,
    model,
    reranker: CrossEncoder,  # ‚Üê –ü–†–ò–ù–ò–ú–ê–ï–ú –ì–û–¢–û–í–´–ô!
    k: int = 5,
    ef: int = 200  # ‚Üê –ü–ê–†–ê–ú–ï–¢–† –ò–ó –†–ï–¢–†–ò–í–ï–†–ê
    ) -> List[Any]:  # ‚Üê –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ö–∏—Ç—ã Milvus, –Ω–µ Document
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞–º + rerank.
    """
    # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–ø—Ä–æ—Å–∞
    paraphrases = generate_paraphrases(original_query)
    related_queries = generate_related_queries(original_query, all_categories)
    all_queries = [original_query] + paraphrases + related_queries

    # 2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏
    query_embs = [model.embed_query(q) for q in all_queries]

    # 3. –ü–æ–∏—Å–∫ –ø–æ –∫–∞–∂–¥–æ–º—É —ç–º–±–µ–¥–¥–∏–Ω–≥—É
    all_results = []
    for emb in query_embs:
        results = collection.search(
            data=[emb],
            anns_field="vector",
            param={"ef": ef},  # ‚Üê –ò–°–ü–û–õ–¨–ó–£–ï–ú –ü–ï–†–ï–î–ê–ù–ù–´–ô ef!
            limit=k,
            output_fields=["title", "text", "url", "subcategory"]
        )
        for hits in results:
            all_results.extend(hits)

    # 4. –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    seen_ids = set()
    unique_results = []
    for hit in all_results:
        if hit.id not in seen_ids:
            seen_ids.add(hit.id)
            unique_results.append(hit)

    # 5. Rerank –æ–¥–Ω–∏–º –∫—Ä–æ—Å—Å-—ç–Ω–∫–æ–¥–µ—Ä–æ–º (–ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–º –∏–∑–≤–Ω–µ!)
    if reranker and len(unique_results) > k:
        pairs = [(original_query, hit.entity.get("text")) for hit in unique_results]
        scores = reranker.predict(pairs)
        scored = sorted(zip(unique_results, scores), key=lambda x: x[1], reverse=True)
        unique_results = [hit for hit, _ in scored[:k]]

    return unique_results  # top_hits

def get_rag_response(question: str) -> str:
    try:
        result = rag_chain_with_sources.invoke(question)
        return result
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏: {e}")
        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    encode_kwargs={'normalize_embeddings': True}
)

vectorstore = connect_to_milvus()
connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
collection = Collection(COLLECTION_NAME)
unique_subcategories = get_unique_subcategories(collection)

retriever = HybridMilvusRetriever(
    vectorstore=vectorstore,
    collection=collection,
    embedding_function=embeddings,
    all_categories=unique_subcategories,
    k=4,
    fetch_k=20,
    ef=200 
)

llm = ChatOpenAI(
    model_name=MODEL_ID,
    openai_api_base=os.environ["OPENAI_API_BASE"],
    openai_api_key=os.environ["OPENAI_API_KEY"],
    #temperature=0.6,
)

template_with_sources = """–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–π –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç.
–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏: "–Ø –Ω–µ –∑–Ω–∞—é –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö".
–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}
–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:"""

prompt = ChatPromptTemplate.from_template(template_with_sources)

rag_chain_with_sources = (
    retrieve_with_sources
    | prompt
    | llm
    | StrOutputParser()
)

if __name__ == "__main__":
    print(">>> –ù–∞—á–∏–Ω–∞–µ–º...")

    while True:
        user_input = input("\n[–¢—ã] ").strip()
        if user_input.lower() in ["–≤—ã—Ö–æ–¥", "quit", "exit"]:
            break

        # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—å –∑–Ω–∞–Ω–∏—è
        result = get_rag_response(user_input)
        print(f"[–°–ò–°–¢–ï–ú–ê] {result}")