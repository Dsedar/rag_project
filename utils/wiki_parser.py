import requests
from bs4 import BeautifulSoup, Comment
import html2text
import re
import json
import time
from typing import List, Dict, Set, Tuple

# ----------------------------
# 1. –ù–ê–°–¢–†–û–ô–ö–ò
# ----------------------------
WIKI_DOMAIN = "vedmak.fandom.com"
ROOT_CATEGORY = "–ö–∞—Ç–µ–≥–æ—Ä–∏—è:–í–µ–¥—å–º–∞–∫ 3"
OUTPUT_FILE = "witcher3_knowledge_base.json"
DELAY = 0.5

# ----------------------------
# 2. –û–ß–ò–°–¢–ö–ê –¢–ï–ö–°–¢–ê
# ----------------------------
def clean_postprocessing(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r"''+'(.*?)''+'", r"\1", text)
    text = re.sub(r"_(.*?)_", r"\1", text)
    text = re.sub(r"&nbsp;", " ", text)
    text = re.sub(r"&mdash;", "‚Äî", text)
    text = re.sub(r"&[a-z]+;", " ", text)
    text = re.sub(r"\.{2,}$", "", text, flags=re.MULTILINE)
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"\n+", "\n", text)
    text = re.sub(r"^\s+|\s+$", "", text, flags=re.M)
    text = re.sub(r"^[.,;:!?‚Äî\-‚Äî\s]+$", "", text, flags=re.M)
    text = re.sub(r"\.{2,}", "...", text)
    return text.strip()

# ----------------------------
# 3. –ü–ê–†–°–ò–ù–ì –°–¢–ê–¢–¨–ò
# ----------------------------
def get_page_content(title: str, wiki_domain: str, subcategory: str) -> Dict[str, str]:
    url = f"https://{wiki_domain}/api.php"
    params = {
        "action": "parse",
        "page": title,
        "format": "json",
        "prop": "text",
        "redirects": True,
    }
    try:
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()
        data = response.json()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è '{title}': {e}")
        return None
    if "error" in data:
        print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ (–æ—à–∏–±–∫–∞ API): {title} ‚Äî {data['error']['info']}")
        return None
    try:
        html_content = data["parse"]["text"]["*"]
    except KeyError:
        print(f"‚ö†Ô∏è –ù–µ—Ç HTML –¥–ª—è: {title}")
        return None
    soup = BeautifulSoup(html_content, "html.parser")
    # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
    for selector in ["script", "style", "nav", "footer", "sup", "table", "aside",
                     "img", "figcaption", ".mw-editsection", ".reference", ".navbox"]:
        for el in soup.select(selector):
            el.decompose()
    # –£–¥–∞–ª–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤
    for comment in soup.find_all(string=lambda s: isinstance(s, Comment)):
        comment.extract()
    # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–∫—Å—Ç
    text_maker = html2text.HTML2Text()
    text_maker.ignore_links = True
    text_maker.ignore_images = True
    text_maker.body_width = 0
    raw_text = text_maker.handle(str(soup))
    clean_text = clean_postprocessing(raw_text)
    return {
        "title": data["parse"]["title"],
        "text": clean_text,
        "url": f"https://{wiki_domain}/wiki/{title.replace(' ', '_')}",
        "subcategory": subcategory  # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—É—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
    }

# ----------------------------
# 4. –†–ï–ö–£–†–°–ò–í–ù–´–ô –ü–ê–†–°–ò–ù–ì –ö–ê–¢–ï–ì–û–†–ò–ô
# ----------------------------
def get_pages_from_category(category: str, wiki_domain: str, limit=500) -> Tuple[List[str], List[str]]:
    url = f"https://{wiki_domain}/api.php"
    params = {
        "action": "query",
        "list": "categorymembers",
        "cmtitle": category,
        "cmlimit": limit,
        "format": "json"
    }
    pages = []
    subcats = []
    while True:
        try:
            response = requests.get(url, params=params, timeout=10)
            data = response.json()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ API –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {category}: {e}")
            break
        for member in data["query"]["categorymembers"]:
            title = member["title"]
            ns = member["ns"]
            if ns == 14:  # –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è
                subcats.append(title)
            elif ns == 0:  # –°—Ç–∞—Ç—å—è
                pages.append(title)
        if "continue" in data:
            params.update(data["continue"])
            time.sleep(DELAY)
        else:
            break
    return pages, subcats

def get_all_pages_recursive(category: str, wiki_domain: str, visited: Set[str] = None, max_depth=5, depth=0) -> Dict[str, Set[str]]:
    if visited is None:
        visited = set()
    if depth >= max_depth:
        return {}
    print(f"{'  ' * depth}üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
    if category in visited:
        return {}
    visited.add(category)
    pages, subcats = get_pages_from_category(category, wiki_domain)

    # –¢–µ–ø–µ—Ä—å —Ö—Ä–∞–Ω–∏–º —Å–ª–æ–≤–∞—Ä—å: title -> set(subcategories)
    result = {}

    print(f"{'  ' * depth}–ù–∞—à–ª–∏ {len(pages)} —Å—Ç–∞—Ç–µ–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category}")
    for page in pages:
        if page not in result:
            result[page] = set()
        result[page].add(category)  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é

    for subcat in subcats:
        if subcat not in visited:
            print(f"{'  ' * depth}–ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {subcat}")
            sub_result = get_all_pages_recursive(subcat, wiki_domain, visited, max_depth, depth + 1)
            for title, subcats_set in sub_result.items():
                if title not in result:
                    result[title] = set()
                result[title].update(subcats_set)

    return result


# ----------------------------
# 5. –û–°–ù–û–í–ù–û–ô –ó–ê–ü–£–°–ö
# ----------------------------
if __name__ == "__main__":
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {ROOT_CATEGORY}")

    title_to_subcats = get_all_pages_recursive(ROOT_CATEGORY, WIKI_DOMAIN)
    all_titles = list(title_to_subcats.keys())
    print(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(all_titles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.\n")

    # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏: –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 20 —Å—Ç–∞—Ç–µ–π —Å –∏—Ö –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏
    print("üìÉ –°–æ–¥–µ—Ä–∂–∏–º–æ–µ title_to_subcats (–ø–µ—Ä–≤—ã–µ 20):")
    for i, (title, subcats) in enumerate(list(title_to_subcats.items())[:20]):
        print(f"{i+1}. {title} ‚Äî –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join(subcats)}")

    # 2. –ü–∞—Ä—Å–∏–º –∫–∞–∂–¥—É—é —Å—Ç–∞—Ç—å—é
    knowledge_base = []
    for i, (title, subcats_set) in enumerate(title_to_subcats.items()):
        subcategory = next(iter(subcats_set))  # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—É—é –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
        print(f"[{i+1}/{len(title_to_subcats)}] –ü–∞—Ä—Å–∏–Ω–≥: {title} (–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {subcategory})")
        content = get_page_content(title, WIKI_DOMAIN, subcategory)
        if content:
            knowledge_base.append(content)
        time.sleep(DELAY)

    # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(knowledge_base, f, ensure_ascii=False, indent=2)
    print(f"\nüéâ –ì–æ—Ç–æ–≤–æ! –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ '{OUTPUT_FILE}'")
    print(f"üì¶ {len(knowledge_base)} —Å—Ç–∞—Ç–µ–π —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ.")

