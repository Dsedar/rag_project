import json
import time
from pathlib import Path
from typing import List, Dict
from sentence_transformers import SentenceTransformer
from pymilvus import (
    connections, FieldSchema, CollectionSchema, DataType,
    Collection, utility, MilvusClient
)
from tqdm import tqdm

# ----------------------------
# 1. –ù–ê–°–¢–†–û–ô–ö–ò
# ----------------------------
DATA_FILE = "witcher3_knowledge_base.json"  # –í—ã—Ö–æ–¥ –ø–∞—Ä—Å–µ—Ä–∞
MILVUS_HOST = "localhost"
MILVUS_PORT = "19530"
COLLECTION_NAME = "witcher3_rag"
EMBEDDING_MODEL = "BAAI/bge-m3"
VECTOR_DIM = 1024

# ----------------------------
# 2. –£–ú–ù–´–ô –ß–ê–ù–ö–ò–ù–ì –ü–û –ê–ë–ó–ê–¶–ê–ú
# ----------------------------
def chunk_by_paragraphs(text: str, max_chars: int = 500, overlap: int = 50) -> List[str]:
    """
    –†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ –∞–±–∑–∞—Ü–∞–º, –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—è –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏.
    """
    paragraphs = text.split('\n')
    chunks = []
    current_chunk = ""
    for para in paragraphs:
        para = para.strip()
        if not para:
            continue
        if len(current_chunk) + len(para) + 1 <= max_chars:
            current_chunk += (" " + para) if current_chunk else para
        else:
            if current_chunk:
                chunks.append(current_chunk)
            if len(para) > max_chars:
                for i in range(0, len(para), max_chars - overlap):
                    chunk_part = para[i:i + max_chars - overlap]
                    chunks.append(chunk_part)
            else:
                current_chunk = para
    if current_chunk:
        chunks.append(current_chunk)
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > 65535:
            final_chunks.append(chunk[:65530] + "... [–æ–±—Ä–µ–∑–∞–Ω–æ]")
        else:
            final_chunks.append(chunk)
    return final_chunks

# ----------------------------
# 3. –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–ï –ö MILVUS
# ----------------------------
def setup_collection():
    connections.connect("default", host=MILVUS_HOST, port=MILVUS_PORT)
    print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Milvus (Docker)")
    if utility.has_collection(COLLECTION_NAME):
        utility.drop_collection(COLLECTION_NAME)
        print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è: {COLLECTION_NAME}")

    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ subcategory
    fields = [
        FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
        FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
        FieldSchema(name="url", dtype=DataType.VARCHAR, max_length=512),
        FieldSchema(name="subcategory", dtype=DataType.VARCHAR, max_length=128),
        FieldSchema(name="vector", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_DIM)
    ]
    schema = CollectionSchema(fields, description="The Witcher 3 RAG")
    collection = Collection(COLLECTION_NAME, schema)

    # HNSW ‚Äî —Ö–æ—Ä–æ—à –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    # M: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É–∑–ª–∞. –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–∏—Å–∫–∞, –Ω–æ –∑–∞–º–µ–¥–ª—è–µ—Ç –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.
    # efConstruction: –†–∞–∑–º–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞. –ß–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ.
    # ef: –†–∞–∑–º–µ—Ä –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ —Å–ø–∏—Å–∫–∞ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –≤–æ –≤—Ä–µ–º—è –ø–æ–∏—Å–∫–∞. –ß–µ–º –±–æ–ª—å—à–µ, —Ç–µ–º —Ç–æ—á–Ω–µ–µ –ø–æ–∏—Å–∫, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ.
    index_params = {
        "index_type": "HNSW",
        "metric_type": "COSINE",
        "params": {"M": 16, "efConstruction": 200, "ef": 100}
    }
    collection.create_index("vector", index_params)
    collection.load()
    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∞ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∞")
    return collection

# ----------------------------
# 4. –ó–ê–ì–†–£–ó–ö–ê –ò –í–°–¢–ê–í–ö–ê
# ----------------------------
def load_and_insert(collection: Collection):
    if not Path(DATA_FILE).exists():
        raise FileNotFoundError(f"–ù–µ—Ç —Ñ–∞–π–ª–∞: {DATA_FILE}")
    
    with open(DATA_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Å—Ç–∞—Ç–µ–π")
    model = SentenceTransformer(EMBEDDING_MODEL)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å {EMBEDDING_MODEL} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

    titles, texts, urls, subcategories, vectors = [], [], [], [], []

    # –°–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –¥–ª—è –≤—Å–µ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞
    with tqdm(total=len(data), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞", unit="item") as pbar:
        for item in data:
            title = item.get('title', 'No Title')
            formatted_title = f"{title[:20]}..." if len(title) > 23 else title
            print(f"\r\033[94m{formatted_title}\033[0m", end="")
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—Ç–∞—Ç—å–∏
            chunks = chunk_by_paragraphs(item["text"], max_chars=3000)
            for chunk in chunks:
                if len(chunk) >= 65535:
                    chunk = chunk[:65000] + "..."
                titles.append(item["title"])
                texts.append(chunk)
                urls.append(item["url"])
                subcategories.append(item.get("subcategory", "unknown"))  # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—é
                emb = model.encode(chunk, normalize_embeddings=True)
                vectors.append(emb.tolist())
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
            pbar.update(1)

    print(f"üîÑ –í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(texts)}")
    print("üì• –í—Å—Ç–∞–≤–∫–∞ –≤ Milvus...")
    collection.insert([titles, texts, urls, subcategories, vectors])
    collection.flush()
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –≤—Å—Ç–∞–≤–ª–µ–Ω–æ {len(texts)} –≤–µ–∫—Ç–æ—Ä–æ–≤")

    # –ñ–¥—ë–º, –ø–æ–∫–∞ –¥–∞–Ω–Ω—ã–µ –±—É–¥—É—Ç –¥–æ—Å—Ç—É–ø–Ω—ã
    print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤—Å—Ç–∞–≤–∫–∏...")
    collection.num_entities

    # –ñ–¥—ë–º, –ø–æ–∫–∞ –∏–Ω–¥–µ–∫—Å –±—É–¥–µ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ—Å—Ç—Ä–æ–µ–Ω
    print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞...")
    utility.wait_for_index_building_complete(COLLECTION_NAME)
    print("‚úÖ –ò–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω")

    # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
    print("üîÅ –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ –ø–∞–º—è—Ç—å...")
    collection.load()
    print("‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è –≥–æ—Ç–æ–≤–∞ –∫ –ø–æ–∏—Å–∫—É")

# ----------------------------
# 5. –ü–†–û–í–ï–†–ö–ê –ü–û–ò–°–ö–ê –° –§–ò–õ–¨–¢–†–û–ú –ü–û –ü–û–î–ö–ê–¢–ï–ì–û–†–ò–ò
# ----------------------------
def test_search(collection: Collection):
    model = SentenceTransformer(EMBEDDING_MODEL)
    query = "–ß—Ç–æ —Ç–∞–∫–æ–µ –î–∏–∫–∞—è –û—Ö–æ—Ç–∞?"
    query_emb = model.encode(query, normalize_embeddings=True).tolist()

    # –ü–æ–∏—Å–∫ —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    search_params = {
        "metric_type": "COSINE",
        "params": {"ef": 200}
    }
    try:
        # –ü—Ä–∏–º–µ—Ä —Ñ–∏–ª—å—Ç—Ä–∞: –∏—Å–∫–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ "–∫–≤–µ—Å—Ç—ã"
        results = collection.search(
            data=[query_emb],
            anns_field="vector",
            param=search_params,
            limit=3,
            #expr='subcategory == "–∫–≤–µ—Å—Ç—ã"',  # –§–∏–ª—å—Ç—Ä –ø–æ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            output_fields=["title", "text", "url", "subcategory"]
        )
        #print(f"\nüîç –ü–æ–∏—Å–∫: '{query}' (—Ñ–∏–ª—å—Ç—Ä: subcategory == '–∫–≤–µ—Å—Ç—ã')")
        for hits in results:
            for hit in hits:
                print(f"\n--- –†–µ–∑—É–ª—å—Ç–∞—Ç {hit.rank} | –°—Ö–æ–∂–µ—Å—Ç—å: {hit.distance:.4f} ---")
                print(f"üìå {hit.entity.get('title')}")
                print(f"üîó {hit.entity.get('url')}")
                print(f"üè∑Ô∏è {hit.entity.get('subcategory')}")
                print(f"üí¨ {hit.entity.get('text')[:400]}...")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")

# ----------------------------
# 6. –ó–ê–ü–£–°–ö
# ----------------------------
if __name__ == "__main__":
    try:
        collection = setup_collection()
        load_and_insert(collection)
        test_search(collection)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    finally:
        connections.disconnect("default")
